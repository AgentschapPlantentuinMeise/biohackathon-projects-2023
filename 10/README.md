# Project 10: Community-driven continuous benchmarking of single-cell tools

## Abstract

Single-cell analysis has emerged as a powerful tool in the study of complex biological systems, but there are currently no standardized platforms for evaluating the accuracy and reproducibility of the wide array of computational tools and workflows. Benchmarking is critical for identifying and addressing the limitations of single-cell analysis tools and workflows, as well as for comparing the performance of different methods. Community engagement in the benchmarking process is important, as it enables researchers, both end-users and methods developers, to share their data, software, and expertise to improve the quality of reference datasets and evaluation metrics. However, current benchmarking efforts for single-cell analysis methods are mostly static and do not support easy extension and updating, a critical need given the fast pace at which new methods are developed.

A recent article in Nature (https://www.nature.com/articles/d41586-022-04426-5) highlights the need to address this issue via implementation of single-cell benchmarking via the OpenEBench (https://openebench.bsc.es) and OMNIBENCHMARK (https://omnibenchmark.org/) platforms. These platforms provide a framework for curating reference datasets, algorithms, and workflows for single-cell analysis, with a view towards extensibility as new computational approaches emerge. These two platforms share some commonalities, yet cater for different target audiences and benchmarking strategies.

Our proposal aims to address these issues by assembling motivated benchmarking-minded computational biologists to push forward existing and new “continuous” benchmarking efforts.

## More information

Timeline:

 * Prepare (data wrangling, organizing speakers, advertising) [2-4 month before BH]
 * Establish a benchmark in OMNIBENCHMARK [during BH]
 * Report experiences and findings in biohackRxiv [6 weeks after BH]
 * Encourage externals to contribute to the SCO OMNIBENCHMARK [Q1 2024]
 * Write article outlining the landscape of systematic benchmarking platforms for SCO in collaboration with OpenEBench [Q1 2024]
   
### Participants

We aim to attract 12 or more participants. We will divide the participants into small working groups: data, methods, and evaluation measures. The minimum requirement to participate is some familiarity with all of the following: command line, GitLab CI/CD, docker, Python, and R.

### Schedule

Prof. Robinson hosted two (hybrid) hackathons for OMNIBENCHMARK (a 2-full-days format). We will extend this setup (lectures, hacking, feedback, and hotfixes) to 1 day of onboarding/training, 2 full days of hacking on the system, and a 1 multi-faceted day to continue hacking and interface with connections to OpenEBench.


## Lead(s)

Ahmed Mahfouz, Mark Robinson, Salvador Capella-Gutierrez


